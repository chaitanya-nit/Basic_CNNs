{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "We will run the LeNet model on MNIST data set which is available in keras datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,AvgPool2D,Conv2D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of x_train : (60000, 28, 28)\nshape of y_train : (60000,)\nshape of x_test : (10000, 28, 28)\nshape of y_test : (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Download dataset\n",
    "(x_train,y_train) , (x_test,y_test) =  mnist.load_data()\n",
    "\n",
    "print(\"shape of x_train :\",x_train.shape)\n",
    "print(\"shape of y_train :\",y_train.shape)\n",
    "print(\"shape of x_test :\",x_test.shape)\n",
    "print(\"shape of y_test :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1B8C34D3BE0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nL2RMUtCYRSGX8MwSChsMRAiaeiGSNDUFDg0Obp0myNbosEf4GwFUX+hrSnwL9wpMA26DhfEsSHCJBoEeZyqm55v9UznnOf7vvf9zpHmH4lYvlM+eWrrZmSdOx0CUDIfybwB8HFo0uoXfeDaNvDMC5C3YaUF4Dm8ZzvAw2+ZjLPjYkFSYF3bDkfwT3PhD3qbi5KkC1Px/BucmrfRqpJ3Kw6zkhJ1og0XTEGYc8EG1KZ7a4++JGl9YIzvnu7BlvaOWnC5NA33A+g1P2H8ujyrdXUGAO/x5s8/a6m0dn0N7E3PMSZiA2Ge8jxMjgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "image.array_to_img(x_train[10].reshape(28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train shape (60000, 28, 28, 1)\nx_test shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "#Scaling the image to 0-->1\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "#Each image should have shape of (28,28,1)\n",
    "x_train = np.expand_dims(x_train,-1)\n",
    "x_test = np.expand_dims(x_test,-1)\n",
    "print(\"X_train shape\",x_train.shape)\n",
    "print(\"x_test shape\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels to categorical \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train,num_classes=10)\n",
    "y_test = to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 24, 24, 6)         156       \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 12, 12, 6)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 8, 8, 16)          2416      \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 4, 4, 16)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 256)               0         \n_________________________________________________________________\ndense (Dense)                (None, 120)               30840     \n_________________________________________________________________\ndense_1 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 44,426\nTrainable params: 44,426\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model building\n",
    "#Lenet uses average pooling and TanH activation function\n",
    "model_lenet = Sequential()\n",
    "model_lenet.add(Conv2D(6,kernel_size=(5,5),activation='tanh',input_shape = x_train[0].shape))\n",
    "model_lenet.add(AvgPool2D(pool_size = (2,2)))\n",
    "model_lenet.add(Conv2D(16,kernel_size = (5,5),activation='tanh'))\n",
    "model_lenet.add(AvgPool2D(pool_size = (2,2)))\n",
    "model_lenet.add(Flatten())\n",
    "model_lenet.add(Dense(120,activation='tanh'))\n",
    "model_lenet.add(Dense(84,activation='tanh'))\n",
    "model_lenet.add(Dense(10,activation='softmax'))\n",
    "model_lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5995 - accuracy: 0.8460 - val_loss: 0.3119 - val_accuracy: 0.9106\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.2741 - accuracy: 0.9201 - val_loss: 0.2212 - val_accuracy: 0.9346\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2013 - accuracy: 0.9405 - val_loss: 0.1647 - val_accuracy: 0.9496\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1569 - accuracy: 0.9542 - val_loss: 0.1309 - val_accuracy: 0.9614\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1281 - accuracy: 0.9626 - val_loss: 0.1074 - val_accuracy: 0.9680\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.1087 - accuracy: 0.9690 - val_loss: 0.0943 - val_accuracy: 0.9711\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0947 - accuracy: 0.9723 - val_loss: 0.0817 - val_accuracy: 0.9746\n",
      "Epoch 8/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0844 - accuracy: 0.9757 - val_loss: 0.0755 - val_accuracy: 0.9757\n",
      "Epoch 9/15\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.0686 - val_accuracy: 0.9777\n",
      "Epoch 10/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0696 - accuracy: 0.9801 - val_loss: 0.0618 - val_accuracy: 0.9795\n",
      "Epoch 11/15\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0641 - accuracy: 0.9816 - val_loss: 0.0569 - val_accuracy: 0.9804\n",
      "Epoch 12/15\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0598 - accuracy: 0.9829 - val_loss: 0.0554 - val_accuracy: 0.9813\n",
      "Epoch 13/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0560 - accuracy: 0.9840 - val_loss: 0.0515 - val_accuracy: 0.9813\n",
      "Epoch 14/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0524 - accuracy: 0.9850 - val_loss: 0.0494 - val_accuracy: 0.9837\n",
      "Epoch 15/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0496 - accuracy: 0.9862 - val_loss: 0.0466 - val_accuracy: 0.9835\n"
     ]
    }
   ],
   "source": [
    "#Compile and model fit\n",
    "model_lenet.compile(optimizer='sgd',loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "hist = model_lenet.fit(x_train,y_train,batch_size=32,epochs=15,validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_lenet,to_file='model_lenet.jpg',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0466 - accuracy: 0.9835\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.04662472382187843, 0.9835000038146973]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model_lenet.evaluate(x_test,y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}